import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import pickle

torch.manual_seed(1234)
np.random.seed(1234)

collection_name = 'sandbar'

with open(f'{collection_name}_dist','rb') as file:
    dist = pickle.load(file)

with open(f'{collection_name}_price','rb') as file:
    price = pickle.load(file)


mat_dist = dist[~np.eye(dist.shape[0],dtype=bool)].reshape(dist.shape[0],-1)

price_ligne = price.T
mat_price_diag = np.tile(price_ligne, (price.size, 1))
mat_price = mat_price_diag[~np.eye(mat_price_diag.shape[0],dtype=bool)].reshape(mat_price_diag.shape[0],-1)


mat_dist_triee = np.empty_like(mat_dist)
mat_price_rearrangee = np.empty_like(mat_price)

for i in range(mat_dist.shape[0]):
    indices_tri = np.argsort(mat_dist[i])
    mat_dist_triee[i] = mat_dist[i, indices_tri]
    mat_price_rearrangee[i] = mat_price[i, indices_tri]

dist_price = np.hstack((mat_dist_triee,mat_price_rearrangee))

# Nombre total de NFTs
number_total = dist_price.shape[0]
number_training = 150
number_test = number_total - number_training


indices = np.arange(number_total)
np.random.shuffle(indices)


idx_train = indices[:number_training]
idx_test = indices[number_training:number_training+number_test]

# Créer les ensembles d'entraînement
train_dist_price = dist_price[idx_train, :]
train_target_price = price[idx_train]

# Créer les ensembles de test
test_dist_price = dist_price[idx_test, :]
test_target_price = price[idx_test]

# Normalisation
scaler = StandardScaler()
#scaler.fit_transform(train_dist_price)
#scaler.transform(test_dist_price)

# création de tenseurs
inputs = torch.tensor(train_dist_price, dtype=torch.float32)
targets = torch.tensor(train_target_price, dtype=torch.float32)

# création d'un dataset
dataset = TensorDataset(inputs, targets)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

class NFTPricePredictor(nn.Module):
    def __init__(self):
        super(NFTPricePredictor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(dist_price.shape[1], 20),
            nn.ReLU(),
            nn.Linear(20, 20),
            nn.ReLU(),
            nn.Linear(20, 20),
            nn.ReLU(),
            nn.Linear(20, 1)
        )
    
    def forward(self, x):
        return self.network(x)

model = NFTPricePredictor()

# Define loss function and optimizer
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# Training loop
epochs = 10000
for epoch in range(epochs):
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred.squeeze(), y)
        
        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')




test_inputs = torch.tensor(test_dist_price, dtype=torch.float32)
test_targets = torch.tensor(test_target_price, dtype=torch.float32)

test_dataset = TensorDataset(test_inputs, test_targets)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # Pas besoin de mélanger pour l'évaluation

# Passer le modèle en mode évaluation
model.eval()

# Accumuler les prédictions et les vraies cibles ici
predictions, actuals = [], []

with torch.no_grad():  # Pas besoin de calculer les gradients pour l'évaluation
    for inputs, targets in test_dataloader:
        # Faire des prédictions
        outputs = model(inputs)
        predictions.extend(outputs.view(-1).tolist())
        actuals.extend(targets.view(-1).tolist())

# Calculer la métrique de performance, par exemple, l'erreur quadratique moyenne (MSE)
mse = mean_absolute_error(actuals, predictions)
print(f'Mean Squared Error on Test Set: {mse}')